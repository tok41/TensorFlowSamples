{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoichi/.pyenv/versions/anaconda2-4.3.0/envs/choco3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Data\n",
    "- tf.Data APIはシンプルなピースで複雑な入力パイプラインを構築できる\n",
    "    - 例えば、分散ファイルシステムを使っている場合に、複数のファイルを集めてランダムなノイズを乗せたり、ミニバッチ用にランダムにセレクトするなどが必要な場合ががある。\n",
    "    - 他の例では、テキストデータの場合、生のテキストデータからシンボルを抽出したり云々。\n",
    "- tf.data API は二つの新しい概念を提供する\n",
    "    - tf.data.Datasetは一連の要素を表す。ここでの要素は、1つ以上のTensorオブジェクトが含まれる。例えば、image pipelineの場合、画像データとラベルを表すテンソルのペア。以下の二つの方法がある\n",
    "        - source(e.g. Dataset.from_tensor_slices()):一つ以上のtf.Tensorオブジェクトからsetasetを生成する\n",
    "        - transformation(e.g. Dataset.batch()):一つ以上のtf.data.Datasetオブジェクトから\n",
    "    - tf.data.Iteratorはdatasetから要素を抽出する主要な方法を示す。Iterator.get_next()によって、Datasetの次の要素が返される。\n",
    "\n",
    "https://www.tensorflow.org/guide/datasets\n",
    "\n",
    "https://www.tensorflow.org/versions/r1.8/programmers_guide/datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic mechanism\n",
    "様々な種類のDatasetオブジェクトとIteratorオブジェクトを作成する基本と、それらからデータを抽出する方法についての解説.\n",
    "\n",
    "- まずデータソースを定義する必要がある\n",
    "    - 例えば、メモリ上のいくつかのテンソルからDatasetを生成するには、tf.data.Dataset.from_tensors() or tf.data.Dataset.from_tensor_slices()を使う。\n",
    "    - または、TFRecord formatでdiskにあるなら、tf.data.TFRecordDataset。\n",
    "\n",
    "- Datasetオブジェクトを作成したら、tf.data.Datasetオブジェクトのメソッドを連させて新しいDatasetに変換できる\n",
    "    - 例えば、Dataset.map(各要素に関数を適用する), Dataset.batch(複数要素変換?)\n",
    "\n",
    "- Datasetからデータを消費する方法は、iteratorを作成すること(tf.data.Iterator)\n",
    "    - Iterator.initializer:iteratorの状態を初期化する\n",
    "    - Iterator.get_next():tf.Tensorオブジェクトを返す。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset structure\n",
    "- datasetは同じ構造の要素の集合で構成される。これらの要素は一つ以上のtf.Tensorオブジェクトでコンポーネントと呼ぶ。\n",
    "- 各コンポーネントはtf.Dtype(テンソルの型を表す)とtf.TensorShape(各要素の形状)を持つ。\n",
    "- Dataset.output_types and Dataset.output_shapes プロパティはデータセット要素の各コンポーネントの推定される型と形状を調べるのに使う\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<dtype: 'float32'>\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "# 単一コンポーネント([size, dimensions])\n",
    "dataset1 = tf.data.Dataset.from_tensor_slices(tf.random_uniform([4, 10]))\n",
    "print(dataset1.output_types)  # ==> \"tf.float32\"\n",
    "print(dataset1.output_shapes)  # ==> \"(10,)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tf.float32, tf.int32)\n",
      "(TensorShape([]), TensorShape([Dimension(100)]))\n"
     ]
    }
   ],
   "source": [
    "# 複数のコンポーネントで構成する\n",
    "dataset2 = tf.data.Dataset.from_tensor_slices(\n",
    "    (tf.random_uniform([4]),\n",
    "     tf.random_uniform([4, 100], maxval=100, dtype=tf.int32)))\n",
    "print(dataset2.output_types)  # ==> \"(tf.float32, tf.int32)\"\n",
    "print(dataset2.output_shapes)  # ==> \"((), (100,))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tf.float32, (tf.float32, tf.int32))\n",
      "(TensorShape([Dimension(10)]), (TensorShape([]), TensorShape([Dimension(100)])))\n"
     ]
    }
   ],
   "source": [
    "dataset3 = tf.data.Dataset.zip((dataset1, dataset2))\n",
    "print(dataset3.output_types)  # ==> (tf.float32, (tf.float32, tf.int32))\n",
    "print(dataset3.output_shapes)  # ==> \"(10, ((), (100,)))\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 各コンポーネントに名前をつけることができる\n",
    "- 辞書の形で名前を指定できる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': tf.float32, 'b': tf.int32}\n",
      "{'a': TensorShape([]), 'b': TensorShape([Dimension(100)])}\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    {\"a\": tf.random_uniform([4]),\n",
    "     \"b\": tf.random_uniform([4, 100], maxval=100, dtype=tf.int32)})\n",
    "print(dataset.output_types)  # ==> \"{'a': tf.float32, 'b': tf.int32}\"\n",
    "print(dataset.output_shapes)  # ==> \"{'a': (), 'b': (100,)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- datasetの変形(変換)\n",
    "- Dataset.map, Dataset.flat_map, Dataset.filter\n",
    "\n",
    "```\n",
    "dataset1 = dataset1.map(lambda x: ...)\n",
    "\n",
    "dataset2 = dataset2.flat_map(lambda x, y: ...)\n",
    "\n",
    "dataset3 = dataset3.filter(lambda x, (y, z): ...)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an iterator\n",
    "- Datasetを作ったら、次は、各要素にアクセスするためのIteratorを作成する\n",
    "- 4つのiteratorをサポートしている\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### one-shot\n",
    "- 最もシンプルなiterator\n",
    "- datasetを一回だけ反復する\n",
    "- Estimatorで簡単に利用できる唯一の型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.range(100)\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "sess = tf.Session()\n",
    "for i in range(100):\n",
    "    value = sess.run(next_element)\n",
    "    assert i == value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### initializable\n",
    "- 使用する前にiterator.initializerを呼び出す必要がある\n",
    "- データセットの定義をiteratorを初期化するときに供給される一つ以上のtf.placeholderテンソルを利用してパラメとライズできる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "max_value = tf.placeholder(tf.int64, shape=[])\n",
    "dataset = tf.data.Dataset.range(max_value)\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "# Initialize an iterator over a dataset with 10 elements.\n",
    "sess.run(iterator.initializer, feed_dict={max_value: 10})\n",
    "for i in range(10):\n",
    "    value = sess.run(next_element)\n",
    "    print(value)\n",
    "    assert i == value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n"
     ]
    }
   ],
   "source": [
    "# Initialize the same iterator over a dataset with 100 elements.\n",
    "sess.run(iterator.initializer, feed_dict={max_value: 100})\n",
    "for i in range(100):\n",
    "    value = sess.run(next_element)\n",
    "    assert i == value\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reinitializable\n",
    "- 複数の異なるDatasetオブジェクトから初期化できる\n",
    "- 例えば、訓練データにランダムな摂動を載せたデータセットと評価用に修正しない画像のデータセットを入力する場合がある。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training and validation datasets with the same structure.\n",
    "training_dataset = tf.data.Dataset.range(100).map(\n",
    "    lambda x: x + tf.random_uniform([], -10, 10, tf.int64))\n",
    "validation_dataset = tf.data.Dataset.range(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tf.int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset.output_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset.output_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A reinitializable iterator is defined by its structure. We could use the\n",
    "# `output_types` and `output_shapes` properties of either `training_dataset`\n",
    "# or `validation_dataset` here, because they are compatible.\n",
    "iterator = tf.data.Iterator.from_structure(training_dataset.output_types,\n",
    "                                           training_dataset.output_shapes)\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "training_init_op = iterator.make_initializer(training_dataset)\n",
    "validation_init_op = iterator.make_initializer(validation_dataset)\n",
    "\n",
    "# Run 20 epochs in which the training dataset is traversed, followed by the\n",
    "# validation dataset.\n",
    "for _ in range(20):\n",
    "    # Initialize an iterator over the training dataset.\n",
    "    sess.run(training_init_op)\n",
    "    for _ in range(100):\n",
    "        sess.run(next_element)\n",
    "\n",
    "    # Initialize an iterator over the validation dataset.\n",
    "    sess.run(validation_init_op)\n",
    "    for _ in range(50):\n",
    "        sess.run(next_element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### feedable\n",
    "- feedable iteratorをtf.placeholderと共に使用するとtf.Session.runの各呼び出しで使用するイテレータを選択できます。\n",
    "- reinitializable iteratorと同じ機能を提供するが、イテレーターを切り替えるときに初期化する必要がない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training and validation datasets with the same structure.\n",
    "training_dataset = tf.data.Dataset.range(100).map(\n",
    "    lambda x: x + tf.random_uniform([], -10, 10, tf.int64)).repeat()\n",
    "validation_dataset = tf.data.Dataset.range(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A feedable iterator is defined by a handle placeholder and its structure. We\n",
    "# could use the `output_types` and `output_shapes` properties of either\n",
    "# `training_dataset` or `validation_dataset` here, because they have\n",
    "# identical structure.\n",
    "handle = tf.placeholder(tf.string, shape=[])\n",
    "iterator = tf.data.Iterator.from_string_handle(\n",
    "    handle, training_dataset.output_types, training_dataset.output_shapes)\n",
    "next_element = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use feedable iterators with a variety of different kinds of iterator\n",
    "# (such as one-shot and initializable iterators).\n",
    "training_iterator = training_dataset.make_one_shot_iterator()\n",
    "validation_iterator = validation_dataset.make_initializable_iterator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The `Iterator.string_handle()` method returns a tensor that can be evaluated\n",
    "# and used to feed the `handle` placeholder.\n",
    "training_handle = sess.run(training_iterator.string_handle())\n",
    "validation_handle = sess.run(validation_iterator.string_handle())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loop forever, alternating between training and validation.\n",
    "while True:\n",
    "    # Run 200 steps using the training dataset. Note that the training dataset is\n",
    "    # infinite, and we resume from where we left off in the previous `while` loop\n",
    "    # iteration.\n",
    "    for _ in range(200):\n",
    "        sess.run(next_element, feed_dict={handle: training_handle})\n",
    "\n",
    "    # Run one pass over the validation dataset.\n",
    "    sess.run(validation_iterator.initializer)\n",
    "    for _ in range(50):\n",
    "        sess.run(next_element, feed_dict={handle: validation_handle})\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consuming values from an iterator\n",
    "- Iterator.getnext()メソッドはイテレーターの次の要素に対応するtf.Tensorオブジェクトを返す\n",
    "- 最後の要素でget_next()メソッドを実行すると、tf.errors.OutOfRangeErrorが返される\n",
    "- 再度このIteratorを使いたい場合には、initializeが必要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(5)\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "# Typically `result` will be the output of a model, or an optimizer's\n",
    "# training operation.\n",
    "result = tf.add(next_element, next_element)\n",
    "\n",
    "sess.run(iterator.initializer)\n",
    "print(sess.run(result))  # ==> \"0\"\n",
    "print(sess.run(result))  # ==> \"2\"\n",
    "print(sess.run(result))  # ==> \"4\"\n",
    "print(sess.run(result))  # ==> \"6\"\n",
    "print(sess.run(result))  # ==> \"8\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of dataset\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    sess.run(result)\n",
    "except tf.errors.OutOfRangeError:\n",
    "    print(\"End of dataset\")  # ==> \"End of dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- datasetの各要素が入れ子構造であるなら、Iterator.get_next()の戻り値は同じ入れ子構造のtf.Tensorオブジェクト\n",
    "- 以下の例では、next1,2,3はIterator.get_next()で生成される同じオペレーションで生成されるテンソル。そのため、これらのテンソルのどれかを評価すると全てのコンポーネントのイテレータが前進する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = tf.data.Dataset.from_tensor_slices(tf.random_uniform([4, 10]))\n",
    "dataset2 = tf.data.Dataset.from_tensor_slices((tf.random_uniform([4]), tf.random_uniform([4, 100])))\n",
    "dataset3 = tf.data.Dataset.zip((dataset1, dataset2))\n",
    "\n",
    "iterator = dataset3.make_initializable_iterator()\n",
    "\n",
    "sess.run(iterator.initializer)\n",
    "next1, (next2, next3) = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'IteratorGetNext_10:0' shape=(10,) dtype=float32>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading input data\n",
    "### Consuming NumPy arrays\n",
    "- 全てのデータがメモリにあるなら、tf.Tensorに変換しDataset.from_tensor_slices()を使って簡単にDatasetを作ることができる\n",
    "- 以下の方法は、tf.constantでデータを定義する。これは、少ないデータなら有効\n",
    "\n",
    "```\n",
    "# Load the training data into two NumPy arrays, for example using `np.load()`.\n",
    "with np.load(\"/var/data/training_data.npy\") as data:\n",
    "  features = data[\"features\"]\n",
    "  labels = data[\"labels\"]\n",
    "\n",
    "# Assume that each row of `features` corresponds to the same row as `labels`.\n",
    "assert features.shape[0] == labels.shape[0]\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tf.placeholder()を使ってDatasetを定義し、numpy arrayを入力することもできる\n",
    "\n",
    "```\n",
    "# Load the training data into two NumPy arrays, for example using `np.load()`.\n",
    "with np.load(\"/var/data/training_data.npy\") as data:\n",
    "  features = data[\"features\"]\n",
    "  labels = data[\"labels\"]\n",
    "\n",
    "# Assume that each row of `features` corresponds to the same row as `labels`.\n",
    "assert features.shape[0] == labels.shape[0]\n",
    "\n",
    "features_placeholder = tf.placeholder(features.dtype, features.shape)\n",
    "labels_placeholder = tf.placeholder(labels.dtype, labels.shape)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features_placeholder, labels_placeholder))\n",
    "# [Other transformations on `dataset`...]\n",
    "dataset = ...\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "\n",
    "sess.run(iterator.initializer, feed_dict={features_placeholder: features,\n",
    "                                          labels_placeholder: labels})\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consuming TFRecord data\n",
    "- tf.data APIはメモリに載り切らない大きいデータセットも扱うことができる\n",
    "- TFRecordフォーマットは行指向のバイナリフォーマット\n",
    "- tf.data.TFRecordDatasetクラスは一つ以上のTFRecordファイルを入力パイプラインの一部として扱える\n",
    "\n",
    "```\n",
    "# Creates a dataset that reads all of the examples from two files.\n",
    "filenames = [\"/var/data/file1.tfrecord\", \"/var/data/file2.tfrecord\"]\n",
    "dataset = tf.data.TFRecordDataset(filenames)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- placeholderでファイル名を入力するノードを作っても良い\n",
    "    - training用とvalidation用で分けるなど"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consuming text data\n",
    "- tf.data.TextLineDataset は一つ以上のテキスト形式のデータファイルからデータレコード(行)を抽出できる、簡単な方法\n",
    "- TextLineDatasetは、1つ以上のファイル名を指定すると、それらのファイルの1行につき1つの文字列値要素を生成する\n",
    "- default動作では、全てのファイルの全ての行を読みだすが、ヘッダーやコメントなどをスキップしたい場合もある。Dataset.skip() and Dataset.filter()を使ってこれらの行を飛ばすことができる。\n",
    "\n",
    "```\n",
    "filenames = [\"/var/data/file1.txt\", \"/var/data/file2.txt\"]\n",
    "dataset = tf.data.TextLineDataset(filenames)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data with Dataset.map()\n",
    "- Dataset.map(f)は関数fをdatasetの各要素に適用することで、あたらしいデータセットを生成する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing tf.Example protocol buffer messages\n",
    "\n",
    "### Decoding image data and resizing it\n",
    "- resizeする\n",
    "\n",
    "### Applying arbitrary Python logic with tf.py_func()\n",
    "- パフォーマンスの理由で、データの前処理はできるだけTensorFlowを使用することが望ましい。\n",
    "- しかし、外部ライブラリを呼び出すことが便利なこともある\n",
    "- このとき、Dataset.map()でtf.py_func()を呼び出す"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching dataset elements\n",
    "\n",
    "### Simple batching\n",
    "- バッチ処理の最も簡単形式は、連続するn個の要素をスタックする\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1, 2, 3]), array([ 0, -1, -2, -3]))\n",
      "(array([4, 5, 6, 7]), array([-4, -5, -6, -7]))\n",
      "(array([ 8,  9, 10, 11]), array([ -8,  -9, -10, -11]))\n"
     ]
    }
   ],
   "source": [
    "inc_dataset = tf.data.Dataset.range(100)\n",
    "dec_dataset = tf.data.Dataset.range(0, -100, -1)\n",
    "dataset = tf.data.Dataset.zip((inc_dataset, dec_dataset))\n",
    "batched_dataset = dataset.batch(4)\n",
    "\n",
    "iterator = batched_dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "print(sess.run(next_element))  # ==> ([0, 1, 2,   3],   [ 0, -1,  -2,  -3])\n",
    "print(sess.run(next_element))  # ==> ([4, 5, 6,   7],   [-4, -5,  -6,  -7])\n",
    "print(sess.run(next_element))  # ==> ([8, 9, 10, 11],   [-8, -9, -10, -11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching tensors with padding\n",
    "- 上の方法は、全て同じサイズのテンソルの場合に使える\n",
    "- 異なるサイズのデータを扱えるモデルがあるので(時系列モデル等)、異なるサイズのデータをバッチ内でパディングして長さを揃えたい場合がある\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0]\n",
      " [1 0 0]\n",
      " [2 2 0]\n",
      " [3 3 3]]\n",
      "[[4 4 4 4 0 0 0]\n",
      " [5 5 5 5 5 0 0]\n",
      " [6 6 6 6 6 6 0]\n",
      " [7 7 7 7 7 7 7]]\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(100)\n",
    "dataset = dataset.map(lambda x: tf.fill([tf.cast(x, tf.int32)], x))\n",
    "dataset = dataset.padded_batch(4, padded_shapes=[None]) # バッチ内でパディング\n",
    "\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "print(sess.run(next_element))  # ==> [[0, 0, 0], [1, 0, 0], [2, 2, 0], [3, 3, 3]]\n",
    "print(sess.run(next_element))  # ==> [[4, 4, 4, 4, 0, 0, 0],\n",
    "                               #      [5, 5, 5, 5, 5, 0, 0],\n",
    "                               #      [6, 6, 6, 6, 6, 6, 0],\n",
    "                               #      [7, 7, 7, 7, 7, 7, 7]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training workflow\n",
    "\n",
    "### Processing multiple epochs\n",
    "- 複数エポック学習するために、データセットを反復処理する方法\n",
    "- Dataset.repeat()を利用する\n",
    "- 引数無しでDataset.repeat()を利用すると、無期限に繰り返される\n",
    "\n",
    "```\n",
    "filenames = [\"/var/data/file1.tfrecord\", \"/var/data/file2.tfrecord\"]\n",
    "dataset = tf.data.TFRecordDataset(filenames)\n",
    "dataset = dataset.map(...)\n",
    "dataset = dataset.repeat(10)\n",
    "dataset = dataset.batch(32)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4]\n",
      "[5 6 7 8 9]\n",
      "[0 1 2 3 4]\n",
      "[5 6 7 8 9]\n",
      "[0 1 2 3 4]\n",
      "[5 6 7 8 9]\n",
      "End of dataset\n"
     ]
    }
   ],
   "source": [
    "inc_dataset = tf.data.Dataset.range(10)\n",
    "inc_dataset = inc_dataset.repeat(3)\n",
    "batched_dataset = inc_dataset.batch(5)\n",
    "\n",
    "iterator = batched_dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        print(sess.run(next_element))\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print(\"End of dataset\")  # ==> \"End of dataset\"\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomly shuffling input data\n",
    "- Dataset.shuffle()を適用すると、入力datasetがランダムにシャッフルされる\n",
    "- tf.RandomShuffleQueueと同じアルゴリズム\n",
    "    - 固定サイズのバッファを保持し、そのバッファからランダムに次の要素を一様に選択"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 8 6 3]\n",
      "[5 4 9 7 2]\n",
      "[4 8 5 0 3]\n",
      "[2 1 7 6 9]\n",
      "[4 0 3 1 9]\n",
      "[7 6 5 8 2]\n",
      "End of dataset\n"
     ]
    }
   ],
   "source": [
    "inc_dataset = tf.data.Dataset.range(10)\n",
    "inc_dataset = inc_dataset.shuffle(buffer_size=1000)\n",
    "inc_dataset = inc_dataset.repeat(3)\n",
    "batched_dataset = inc_dataset.batch(5)\n",
    "\n",
    "iterator = batched_dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        print(sess.run(next_element))\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print(\"End of dataset\")  # ==> \"End of dataset\"\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
