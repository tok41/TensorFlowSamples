{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eager Execution\n",
    "- eager execution -> chainerのようにDefine-by-Runでモデルを記述する\n",
    "- 計算グラフの構築をせずに、即座にオペレーションの評価をする計算環境\n",
    "- eager executionは、実験のための柔軟なプラットフォーム\n",
    "    - 直感的\n",
    "    - 簡単なデバッグ\n",
    "    - 自然な計算フロー:pythonのコードと同じように書ける\n",
    "\n",
    "https://www.tensorflow.org/guide/eager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# basic usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoichi/.pyenv/versions/anaconda2-4.3.0/envs/choco3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.enable_eager_execution() # eager executionの実行用\n",
    "# 計算グラフを構築しないで、すぐに値を返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello, [[4.]]\n"
     ]
    }
   ],
   "source": [
    "x = [[2.]]\n",
    "m = tf.matmul(x, x)\n",
    "print(\"hello, {}\".format(m)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Numpyはtf.Tensorを扱える\n",
    "- TensorFlowのmath operationはNumpyArrayに変換可能(tf.Tensor.numpy method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1 2]\n",
      " [3 4]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([[1, 2],\n",
    "                 [3, 4]])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[2 3]\n",
      " [4 5]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "b = tf.add(a, 1)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  6]\n",
      " [12 20]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# a,bはtf.Tensor\n",
    "c = np.multiply(a, b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'EagerTensor'>\n"
     ]
    }
   ],
   "source": [
    "print(type(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]]\n"
     ]
    }
   ],
   "source": [
    "# Numpy.arrayに変換\n",
    "print(a.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic control flow\n",
    "- eager executionの嬉しいところは、ホスト言語の機能を利用可能なところ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fizzbuzz(max_num):\n",
    "    counter = tf.constant(0)\n",
    "    max_num = tf.convert_to_tensor(max_num)\n",
    "    for num in range(max_num.numpy()):\n",
    "        num = tf.constant(num)\n",
    "        if int(num % 3) == 0 and int(num % 5) == 0:\n",
    "            print('FizzBuzz')\n",
    "        elif int(num % 3) == 0:\n",
    "            print('Fizz')\n",
    "        elif int(num % 5) == 0:\n",
    "            print('Buzz')\n",
    "        else:\n",
    "            print(num)\n",
    "        counter += 1\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FizzBuzz\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "Fizz\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "Buzz\n",
      "Fizz\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "Fizz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=98, shape=(), dtype=int32, numpy=10>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fizzbuzz(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a model\n",
    "- モデルは層を重ねて表現する\n",
    "- eager executionを利用する場合、自分で層を書くか、kerasで提供される層を利用する\n",
    "- 層を自分で書く場合\n",
    "    - tf.keras.layers.Layerを継承すると便利"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySimpleLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, output_units):\n",
    "        super(MySimpleLayer, self).__init__()\n",
    "        self.output_units = output_units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # The build method gets called the first time your layer is used.\n",
    "        # Creating variables on build() allows you to make their shape depend\n",
    "        # on the input shape and hence removes the need for the user to specify\n",
    "        # full shapes. It is possible to create variables during __init__() if\n",
    "        # you already know their full shapes.\n",
    "        self.kernel = self.add_variable(\n",
    "            \"kernel\", [input_shape[-1], self.output_units])\n",
    "    \n",
    "    def call(self, input):\n",
    "        # Override call() instead of __call__ so we can perform some bookkeeping.\n",
    "        return tf.matmul(input, self.kernel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 以下では、上記のMySimpleLayerの代わりにtf.keras.layers.Denseを利用する。\n",
    "- 層を線形にスタックしていくためにtf.keras.Sequentialが使える"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(10, input_shape=(784,)),  # must declare input shape\n",
    "  tf.keras.layers.Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- または、tf.keras.Modelを継承してモデルを構成することもできる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MNISTModel, self).__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(units=10)\n",
    "        self.dense2 = tf.keras.layers.Dense(units=10)\n",
    "\n",
    "    def call(self, input):\n",
    "        \"\"\"Run the model.\"\"\"\n",
    "        result = self.dense1(input)\n",
    "        result = self.dense2(result)\n",
    "        result = self.dense2(result)  # reuse variables from dense2 layer\n",
    "        return result\n",
    "\n",
    "\n",
    "model = MNISTModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eager training\n",
    "## Computing gradients\n",
    "- 自動微分は機械学習ではとっても便利\n",
    "- eager executionでは、tf.GradientTapeを使用して勾配を計算できる\n",
    "- 呼び出し中に異なる動作が発生する可能性があるため、全てのForwardパスの操作は「テープ」に記録される\n",
    "- 勾配を計算する際には、テープを再生する\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "w = tf.contrib.eager.Variable([[1.0]])\n",
    "with tf.GradientTape() as tape:\n",
    "    loss = w * w\n",
    "\n",
    "grad = tape.gradient(loss, w)\n",
    "print(grad)  # => tf.Tensor([[ 2.]], shape=(1, 1), dtype=float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- シンプルなモデルを訓練する例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 69.257\n",
      "Loss at step 000: 66.539\n",
      "Loss at step 020: 30.073\n",
      "Loss at step 040: 13.889\n",
      "Loss at step 060: 6.707\n",
      "Loss at step 080: 3.519\n",
      "Loss at step 100: 2.105\n",
      "Loss at step 120: 1.477\n",
      "Loss at step 140: 1.198\n",
      "Loss at step 160: 1.075\n",
      "Loss at step 180: 1.020\n",
      "Final loss: 0.996\n",
      "W = 3.036214590072632, B = 2.1418139934539795\n"
     ]
    }
   ],
   "source": [
    "# A toy dataset of points around 3 * x + 2\n",
    "NUM_EXAMPLES = 1000\n",
    "training_inputs = tf.random_normal([NUM_EXAMPLES])\n",
    "noise = tf.random_normal([NUM_EXAMPLES])\n",
    "training_outputs = training_inputs * 3 + 2 + noise\n",
    "\n",
    "\n",
    "def prediction(input, weight, bias):\n",
    "    return input * weight + bias\n",
    "\n",
    "\n",
    "# A loss function using mean-squared error\n",
    "def loss(weights, biases):\n",
    "    error = prediction(training_inputs, weights, biases) - training_outputs\n",
    "    # Forward計算が含まれる\n",
    "    return tf.reduce_mean(tf.square(error))\n",
    "\n",
    "\n",
    "# Return the derivative of loss with respect to weight and bias\n",
    "def grad(weights, biases):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # forward計算をテープに記録する\n",
    "        loss_value = loss(weights, biases)\n",
    "    # テープを再生して勾配を計算する\n",
    "    return tape.gradient(loss_value, [weights, biases])\n",
    "\n",
    "\n",
    "\n",
    "train_steps = 200\n",
    "learning_rate = 0.01\n",
    "# Start with arbitrary values for W and B on the same batch of data\n",
    "W = tf.contrib.eager.Variable(5.)\n",
    "B = tf.contrib.eager.Variable(10.)\n",
    "\n",
    "print(\"Initial loss: {:.3f}\".format(loss(W, B)))\n",
    "\n",
    "for i in range(train_steps):\n",
    "    dW, dB = grad(W, B)\n",
    "    W.assign_sub(dW * learning_rate)\n",
    "    B.assign_sub(dB * learning_rate)\n",
    "    if i % 20 == 0:\n",
    "        print(\"Loss at step {:03d}: {:.3f}\".format(i, loss(W, B)))\n",
    "\n",
    "print(\"Final loss: {:.3f}\".format(loss(W, B)))\n",
    "print(\"W = {}, B = {}\".format(W.numpy(), B.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train a model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- trainingを除いて、modelをcallして出力を確認できる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 784)\n",
      "tf.Tensor([[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]], shape=(1, 1, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor representing a blank image\n",
    "batch = tf.zeros([1, 1, 784])\n",
    "print(batch.shape)  # => (1, 1, 784)\n",
    "\n",
    "# call MNIST model\n",
    "result = model(batch)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ここのexampleでは、dataset.py moduleを利用する(直下にこのmoduleをdownloadしておく)\n",
    "- また、MNISTデータを以下のコマンドで取得する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset  # download dataset.py file\n",
    "dataset_train = dataset.train('./datasets').shuffle(60000).repeat(4).batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- modelを訓練するために、loss関数を定義する\n",
    "- 変数の更新のために、optimizerを利用する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 0.227\n",
      "Loss at step 0000: 0.722\n",
      "Loss at step 0200: 0.410\n",
      "Loss at step 0400: 0.886\n",
      "Loss at step 0600: 0.532\n",
      "Loss at step 0800: 0.543\n",
      "Loss at step 1000: 0.365\n",
      "Loss at step 1200: 0.421\n",
      "Loss at step 1400: 0.235\n",
      "Loss at step 1600: 0.567\n",
      "Loss at step 1800: 0.414\n",
      "Loss at step 2000: 0.486\n",
      "Loss at step 2200: 0.499\n",
      "Loss at step 2400: 0.809\n",
      "Loss at step 2600: 0.349\n",
      "Loss at step 2800: 0.437\n",
      "Loss at step 3000: 0.617\n",
      "Loss at step 3200: 0.497\n",
      "Loss at step 3400: 0.394\n",
      "Loss at step 3600: 0.730\n",
      "Loss at step 3800: 0.545\n",
      "Loss at step 4000: 0.676\n",
      "Loss at step 4200: 0.458\n",
      "Loss at step 4400: 0.320\n",
      "Loss at step 4600: 0.285\n",
      "Loss at step 4800: 0.327\n",
      "Loss at step 5000: 0.335\n",
      "Loss at step 5200: 0.281\n",
      "Loss at step 5400: 0.314\n",
      "Loss at step 5600: 0.196\n",
      "Loss at step 5800: 0.272\n",
      "Loss at step 6000: 0.522\n",
      "Loss at step 6200: 0.582\n",
      "Loss at step 6400: 0.241\n",
      "Loss at step 6600: 0.585\n",
      "Loss at step 6800: 0.193\n",
      "Loss at step 7000: 0.253\n",
      "Loss at step 7200: 0.328\n",
      "Loss at step 7400: 0.195\n",
      "Final loss: 0.376\n"
     ]
    }
   ],
   "source": [
    "def loss(model, x, y):\n",
    "    prediction = model(x)\n",
    "    return tf.losses.sparse_softmax_cross_entropy(labels=y, logits=prediction)\n",
    "\n",
    "\n",
    "def grad(model, inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss(model, inputs, targets)\n",
    "    return tape.gradient(loss_value, model.variables)\n",
    "\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "\n",
    "x, y = iter(dataset_train).next()\n",
    "print(\"Initial loss: {:.3f}\".format(loss(model, x, y)))\n",
    "\n",
    "# Training loop\n",
    "for (i, (x, y)) in enumerate(dataset_train):\n",
    "    # Calculate derivatives of the input function with respect to its parameters.\n",
    "    grads = grad(model, x, y)\n",
    "    # Apply the gradient to the model\n",
    "    optimizer.apply_gradients(zip(grads, model.variables),\n",
    "                              global_step=tf.train.get_or_create_global_step())\n",
    "    if i % 200 == 0:\n",
    "        print(\"Loss at step {:04d}: {:.3f}\".format(i, loss(model, x, y)))\n",
    "\n",
    "print(\"Final loss: {:.3f}\".format(loss(model, x, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GPUを使って高速に計算するには"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 0.428\n"
     ]
    }
   ],
   "source": [
    "dataset_train = dataset.train('./datasets').shuffle(60000).repeat(4).batch(32)\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "\n",
    "x, y = iter(dataset_train).next()\n",
    "print(\"Initial loss: {:.3f}\".format(loss(model, x, y)))\n",
    "\n",
    "\n",
    "with tf.device(\"/gpu:0\"):\n",
    "    for (i, (x, y)) in enumerate(dataset_train):\n",
    "        # minimize() is equivalent to the grad() and apply_gradients() calls.\n",
    "        optimizer.minimize(lambda: loss(model, x, y),\n",
    "                           global_step=tf.train.get_or_create_global_step())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables and optimizers\n",
    "- tf.contrib.eager.Variableはtf.Tensorを格納している。自動微分を容易にするために。\n",
    "- モデルのパラメータは変数としてクラスにカプセル化できる\n",
    "- tf.GradientTapeでtf.contrib.eager.Variableを使用することでより良いカプセル化ができる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.contrib.eager as tfe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 69.338\n",
      "Loss at step 000: 66.639\n",
      "Loss at step 020: 30.323\n",
      "Loss at step 040: 14.105\n",
      "Loss at step 060: 6.863\n",
      "Loss at step 080: 3.629\n",
      "Loss at step 100: 2.184\n",
      "Loss at step 120: 1.539\n",
      "Loss at step 140: 1.251\n",
      "Loss at step 160: 1.122\n",
      "Loss at step 180: 1.065\n",
      "Loss at step 200: 1.039\n",
      "Loss at step 220: 1.028\n",
      "Loss at step 240: 1.022\n",
      "Loss at step 260: 1.020\n",
      "Loss at step 280: 1.019\n",
      "Final loss: 1.019\n",
      "W = 2.9842734336853027, B = 1.9938817024230957\n"
     ]
    }
   ],
   "source": [
    "class Model(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.W = tfe.Variable(5., name='weight')\n",
    "        self.B = tfe.Variable(10., name='bias')\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        return inputs * self.W + self.B\n",
    "\n",
    "\n",
    "# A toy dataset of points around 3 * x + 2\n",
    "NUM_EXAMPLES = 2000\n",
    "training_inputs = tf.random_normal([NUM_EXAMPLES])\n",
    "noise = tf.random_normal([NUM_EXAMPLES])\n",
    "training_outputs = training_inputs * 3 + 2 + noise\n",
    "\n",
    "# The loss function to be optimized\n",
    "\n",
    "# loss functionはMSE\n",
    "def loss(model, inputs, targets):\n",
    "    error = model.predict(inputs) - targets\n",
    "    return tf.reduce_mean(tf.square(error))\n",
    "\n",
    "\n",
    "def grad(model, inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss(model, inputs, targets)\n",
    "    return tape.gradient(loss_value, [model.W, model.B])\n",
    "\n",
    "\n",
    "# Define:\n",
    "# 1. A model.\n",
    "# 2. Derivatives of a loss function with respect to model parameters.\n",
    "# 3. A strategy for updating the variables based on the derivatives.\n",
    "model = Model()\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "\n",
    "print(\"Initial loss: {:.3f}\".format(\n",
    "    loss(model, training_inputs, training_outputs)))\n",
    "\n",
    "# Training loop\n",
    "for i in range(300):\n",
    "    grads = grad(model, training_inputs, training_outputs)\n",
    "    optimizer.apply_gradients(zip(grads, [model.W, model.B]),\n",
    "                              global_step=tf.train.get_or_create_global_step())\n",
    "    if i % 20 == 0:\n",
    "        print(\"Loss at step {:03d}: {:.3f}\".format(\n",
    "            i, loss(model, training_inputs, training_outputs)))\n",
    "\n",
    "print(\"Final loss: {:.3f}\".format(\n",
    "    loss(model, training_inputs, training_outputs)))\n",
    "print(\"W = {}, B = {}\".format(model.W.numpy(), model.B.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use objects for state during eager execution\n",
    "- graph execution(define-and-run)の場合、状態(変数)の管理はtf.Sessionによって行われていた\n",
    "- 対して、eager executionでは、変数のlifetimeはpythonオブジェクトのライフタイムによって定義される"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables are objects\n",
    "- eager executionでは、オブジェクトの最後の参照が削除されるまで存続する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"gpu:0\"):\n",
    "    v = tfe.Variable(tf.random_normal([1000, 1000]))\n",
    "    v = None  # v no longer takes up GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object-based saving\n",
    "- tfe.Checkpouintはtfe.Variableのsaveとrestoreができる\n",
    "- tfe.Checkpointでは、model, optimizerなども保存できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=10.0>\n",
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=2.0>\n",
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=11.0>\n",
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=2.0>\n"
     ]
    }
   ],
   "source": [
    "x = tfe.Variable(10.)\n",
    "print(x)\n",
    "\n",
    "checkpoint = tfe.Checkpoint(x=x)  # save as \"x\"\n",
    "\n",
    "x.assign(2.)   # Assign a new value to the variables and save.\n",
    "save_path = checkpoint.save('./ckpt/')\n",
    "print(x)\n",
    "\n",
    "x.assign(11.)  # Change the variable after saving.\n",
    "print(x)\n",
    "\n",
    "# Restore values from the checkpoint\n",
    "checkpoint.restore(save_path)\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object-oriented metrics\n",
    "- tfe.metricsはオブジェクトとして格納される\n",
    "- 新しいデータによってmetricを更新し、tfe.metrics.resultメソッドを利用して結果を取り出す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(2.5, shape=(), dtype=float64)\n",
      "tf.Tensor(5.5, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "m = tfe.metrics.Mean(\"loss\")\n",
    "m(0)\n",
    "m(5) # (0+5)/2\n",
    "print(m.result())\n",
    "m([8, 9]) # (0+5+8+9)/4\n",
    "print(m.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summaries and TensorBoard\n",
    "- tf.contrib.summaryはgraph executionもeager executionも共に利用できる\n",
    "- 以下のように書く\n",
    "\n",
    "```\n",
    "writer = tf.contrib.summary.create_file_writer(logdir)\n",
    "global_step=tf.train.get_or_create_global_step()  # return global step var\n",
    "\n",
    "writer.set_as_default()\n",
    "\n",
    "for _ in range(iterations):\n",
    "  global_step.assign_add(1)\n",
    "  # Must include a record_summaries method\n",
    "  with tf.contrib.summary.record_summaries_every_n_global_steps(100):\n",
    "    # your model code goes here\n",
    "    tf.contrib.summary.scalar('loss', loss)\n",
    "     ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced automatic differentiation topics\n",
    "## Dynamic models\n",
    "- tf.GradientTape は動的なモデルも扱うことができる\n",
    "- example:backtracking line search algorithm\n",
    "    - 最適化の手法の一つ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_search_step(fn, init_x, rate=1.0):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Variables are automatically recorded, but manually watch a tensor\n",
    "        tape.watch(init_x)\n",
    "        value = fn(init_x)\n",
    "    grad = tape.gradient(value, init_x)\n",
    "    grad_norm = tf.reduce_sum(grad * grad)\n",
    "    init_value = value\n",
    "    while value > init_value - rate * grad_norm:\n",
    "        x = init_x - rate * grad\n",
    "        value = fn(x)\n",
    "        rate /= 2.0\n",
    "    return x, value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional functions to compute gradients\n",
    "- tf.GradientTapeは強力なインターフェース。\n",
    "\n",
    "### tfe.gradients_function\n",
    "- 引数の関数の導関数を計算する関数を返す。\n",
    "- 入力関数はスカラ値を返す必要がある\n",
    "- 戻された関数が実行されると、tf.Tensorのリストが返される：入力関数の各引数にそれぞれ対応する要素\n",
    "\n",
    "### tfe.value_and_gradients_function\n",
    "- tfe.gradients_functionと似ているが、返された関数が実行されると、tf.Tensorのリストに加えて、入力関数からの値を返す。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(9.0, shape=(), dtype=float32)\n",
      "[<tf.Tensor: id=1978883, shape=(), dtype=float32, numpy=6.0>]\n",
      "[<tf.Tensor: id=1978895, shape=(), dtype=float32, numpy=2.0>]\n",
      "[None]\n",
      "[<tf.Tensor: id=159, shape=(), dtype=float32, numpy=1.0>]\n",
      "[<tf.Tensor: id=1978920, shape=(), dtype=float32, numpy=-1.0>]\n"
     ]
    }
   ],
   "source": [
    "def square(x):\n",
    "    return tf.multiply(x, x)\n",
    "\n",
    "\n",
    "grad = tfe.gradients_function(square)\n",
    "\n",
    "print(square(3.))  # => 9.0\n",
    "print(grad(3.))    # => [6.0] (x^2)'=2x\n",
    "\n",
    "# The second-order derivative of square:\n",
    "gradgrad = tfe.gradients_function(lambda x: grad(x)[0]) # スカラ値を返す必要?\n",
    "print(gradgrad(3.))  # => [2.0]\n",
    "\n",
    "# The third-order derivative is None:\n",
    "gradgradgrad = tfe.gradients_function(lambda x: gradgrad(x)[0])\n",
    "print(gradgradgrad(3.))  # => [None]\n",
    "\n",
    "\n",
    "# With flow control:\n",
    "def abs(x):\n",
    "    return x if x > 0. else -x\n",
    "\n",
    "\n",
    "grad = tfe.gradients_function(abs)\n",
    "\n",
    "print(grad(3.))   # => [1.0]\n",
    "print(grad(-3.))  # => [-1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom gradients\n",
    "- custom gradients はgradientsをオーバーライドする簡単な方法\n",
    "- forward関数内で入力、出力、または中間結果に関する勾配を定義する。\n",
    "- 以下の例は、backwardパス内で勾配のノルムをクリップする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.custom_gradient\n",
    "def clip_gradient_by_norm(x, norm):\n",
    "    y = tf.identity(x)\n",
    "\n",
    "    def grad_fn(dresult):\n",
    "        return [tf.clip_by_norm(dresult, norm), None]\n",
    "    return y, grad_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- custom gradientsは数値的に安定した勾配を提供するために使われる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: id=1978929, shape=(), dtype=float32, numpy=0.5>]\n",
      "[<tf.Tensor: id=1978938, shape=(), dtype=float32, numpy=nan>]\n"
     ]
    }
   ],
   "source": [
    "def log1pexp(x):\n",
    "    return tf.log(1 + tf.exp(x))\n",
    "\n",
    "\n",
    "grad_log1pexp = tfe.gradients_function(log1pexp)\n",
    "\n",
    "# The gradient computation works fine at x = 0.\n",
    "print(grad_log1pexp(0.))  # => [0.5]\n",
    "\n",
    "# However, x = 100 fails because of numerical instability.\n",
    "print(grad_log1pexp(100.)) # 数値的に計算できる範囲を超えてしまった"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 上記のlog1pexpはcustom gradientsで解析的に簡略化できる\n",
    "- 以下の実装では、forward pathで計算されるtf.exp(x)を再利用する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: id=1978948, shape=(), dtype=float32, numpy=0.5>]\n",
      "[<tf.Tensor: id=1978958, shape=(), dtype=float32, numpy=1.0>]\n"
     ]
    }
   ],
   "source": [
    "@tf.custom_gradient\n",
    "def log1pexp(x):\n",
    "    e = tf.exp(x)\n",
    "\n",
    "    def grad(dy):\n",
    "        return dy * (1 - 1 / (1 + e))\n",
    "    return tf.log(1 + e), grad\n",
    "\n",
    "\n",
    "grad_log1pexp = tfe.gradients_function(log1pexp)\n",
    "\n",
    "# As before, the gradient computation works fine at x = 0.\n",
    "print(grad_log1pexp(0.))  # => [0.5]\n",
    "\n",
    "# And the gradient computation also works at x = 100.\n",
    "print(grad_log1pexp(100.))  # => [1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
